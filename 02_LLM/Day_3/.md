# OLLAMA + LM Studio :


- what is ollama?
  - it is a command line tool 
  - easy to run open source llms.
  - simple cli tool to run LLMs loclly.
  - think of it as "docker for llms"
  - download models once and use it offline.
  - great for privacy ,low latency and cost savings.


- WSL -> Windows Subsystem for Linus is a feature of windows that allows users to run alinux environment on windows without the need for a virtual machine or dual-booting .



 
## ollama cheatsheet :

-  command                   ->   discription
1. ollama pull model         ->   download the model
2. ollama run model         ->   Run the model in terminal
3. ollama list         ->   show installed models.
4. ollama rm model       ->  remove a model.
5. curl localhost:11434/api/generate     ->  access model via HTTP.




  
  ## ollama vs hugging face :

  #### Hugging face :
   - A massive open-source model hub for ML/LLMS
   - model runs -> mostly cloud based or via colab/local python
   - Model format -> transformers (pytorch,tensorFlow,Safetensors,etc.).
   - interface -> python based.
   - hosting -> hosts thousands of model by researchers and orgs.
   - usage ->  best for research,training and online inference.
   - ease of use -> requires python setup and environment .



#### OLLAMA :
- A local LLM running focused on running models on your own machine.
- models run entirely local
- ollama's own format (gguf under the hood).
- interface -> CLI + REST API 
- hosting -> downloads from curated model registry
- best for private,offline interface
- ease of use -> one line install, model ready with ollama run.








# LM Studio :
-  it is a free desktop app that lets you run large llm offline, using a simple chat interface - no coding or terminal required.
- it is like running chatgpt or claude locally on your laptop.




